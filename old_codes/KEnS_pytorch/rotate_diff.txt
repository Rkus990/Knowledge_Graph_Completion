diff --git a/run_kg_only.py b/run_kg_only.py
index d574041..e3d69d9 100644
--- a/run_kg_only.py
+++ b/run_kg_only.py
@@ -199,7 +199,7 @@ def main(args):
     ############ CPU AND GPU related, Mode related, Dataset Related
     if torch.cuda.is_available():
         print("Using GPU" + "-" * 80)
-        args.device = torch.device("cuda:0")
+        args.device = torch.device("cuda:2")
     else:
         print("Using CPU" + "-" * 80)
         args.device = torch.device("cpu")
diff --git a/run_no_generation.py b/run_no_generation.py
index da68e1c..ceac723 100644
--- a/run_no_generation.py
+++ b/run_no_generation.py
@@ -183,13 +183,13 @@ def train_kg_batch(kg,optimizer,num_epoch,device):
 
 def main(args):
     ############ CPU AND GPU related, Mode related, Dataset Related
-    # if torch.cuda.is_available():
-    #     print("Using GPU" + "-" * 80)
-    #     args.device = torch.device("cuda:0")
-    # else:
-    #     print("Using CPU" + "-" * 80)
-    #     args.device = torch.device("cpu")
-    args.device = torch.device("cpu")
+    if torch.cuda.is_available():
+        print("Using GPU" + "-" * 80)
+        args.device = torch.device("cuda:2")
+    else:
+        print("Using CPU" + "-" * 80)
+        args.device = torch.device("cpu")
+    # args.device = torch.device("cpu")
 
 
     set_params(args)
@@ -203,7 +203,7 @@ def main(args):
 
     # load data
     data_dir = './data/kg'  # where you put kg data
-    seed_dir = './data/seed_alignlinks'  # where you put seed align links data
+    seed_dir = './data/seed_alignlinks2'  # where you put seed align links data
     model_dir = join('./trained_model_no_generation', f'kens-{param.knowledge}-{param.dim}', target_lang)  # output
     if not os.path.exists(model_dir):
         os.makedirs(model_dir)
diff --git a/src/knowledgegraph_pytorch.py b/src/knowledgegraph_pytorch.py
index 06e9942..b2d675d 100644
--- a/src/knowledgegraph_pytorch.py
+++ b/src/knowledgegraph_pytorch.py
@@ -8,6 +8,7 @@ import json
 import torch
 from torch.utils.data import DataLoader
 import torch.nn as nn
+import pudb
 
 class KnowledgeGraph(nn.Module):
     def __init__(self, lang, kg_train_data, kg_val_data, dict0to1, dict1to0, num_entity, num_relation,device = torch.device("cpu")):
@@ -148,10 +149,16 @@ class KnowledgeGraph(nn.Module):
                 return self.filtered_reordered_embedding_matrix
 
         E1 = self.model.entity_embedding_layer  # original embedding matrix. shape [num_entity1, dim]
-        E0 = torch.zeros([num_entity0, param.dim]).to(E1.weight.device)  # shape [num_entity0, dim]
+        try:
+            E0 = torch.zeros([num_entity0, param.dim]).to(E1.weight.device)  # shape [num_entity0, dim]
+        except:
+            E0 = torch.zeros([num_entity0, param.dim]).to(E1.device)  # shape [num_entity0, dim]
         # If an entity in target kg is not linked to the current kg, the vector stays default (all 0)
         for e0, e1 in self.dict0to1.items():
-            E0[e0,:] = E1[e1,:]
+            try:
+                E0[e0,:] = E1[e1,:]
+            except:
+                continue
         self.filtered_reordered_embedding_matrix = E0
         return E0
 
@@ -197,15 +204,15 @@ class KnowledgeGraph(nn.Module):
 
 
         self.filtered_reordered_embedding_matrix = None
-        if (kg0.lang, self.lang) in seed_alignlinks:
-            links = seed_alignlinks[(kg0.lang, self.lang)]
+        if (kg0.lang, self.lang) in seed_alignlinks[1]:
+            links = seed_alignlinks[1][(kg0.lang, self.lang)]
             for pair in links:
                 e0, e1 = pair[0].detach().data.item(), pair[1].detach().data.item()
                 if e0 not in self.dict0to1 and e1 not in self.dict1to0:
                     self.dict0to1[e0] = e1
                     self.dict1to0[e1] = e0
         else:
-            links = seed_alignlinks[(self.lang, kg0.lang)]
+            links = seed_alignlinks[1][(self.lang, kg0.lang)]
             for pair in links:
                 e0, e1 = pair[1].detach().data.item(), pair[0].detach().data.item()
                 if e0 not in self.dict0to1 and e1 not in self.dict1to0:
diff --git a/src/model_pytorch.py b/src/model_pytorch.py
index 0b56aba..3f2bb89 100644
--- a/src/model_pytorch.py
+++ b/src/model_pytorch.py
@@ -4,6 +4,7 @@ import numpy as np
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
+import pudb
 
 
 
@@ -64,7 +65,7 @@ class KGFunction(nn.Module):
             rand_negs = torch.randint(high=self.num_entity, size=(batch_size_each,),
                                   device=self.device)  # [b,num_neg]
 
-            neg_each = self.entity_embedding_layer(rand_negs).unsqueeze(1)
+            neg_each = self.entity_embedding_layer[rand_negs].unsqueeze(1)
 
             neg_all.append(neg_each)
 
@@ -82,8 +83,7 @@ class KGFunction(nn.Module):
         #     dim=0,
         #     index=sample[:, 0]
         # ).unsqueeze(1)
-
-        h = self.entity_embedding_layer(sample[:,0]).unsqueeze(1)
+        h = self.entity_embedding_layer[sample[:,0]].unsqueeze(1)
 
         # r = torch.index_select(
         #     self.rel_embedding_layer,
@@ -91,14 +91,14 @@ class KGFunction(nn.Module):
         #     index=sample[:, 1]
         # ).unsqueeze(1)
 
-        r = self.rel_embedding_layer(sample[:, 1]).unsqueeze(1)
+        r = self.rel_embedding_layer[sample[:, 1]].unsqueeze(1)
 
         # t = torch.index_select(
         #     self.entity_embedding_layer,
         #     dim=0,
         #     index=sample[:, 2]
         # ).unsqueeze(1)
-        t = self.entity_embedding_layer(sample[:, 2]).unsqueeze(1)
+        t = self.entity_embedding_layer[sample[:, 2]].unsqueeze(1)
 
 
         projected_t = project_t([h, r],self.device) ##
@@ -155,9 +155,9 @@ class KGFunction(nn.Module):
         #     index=r
         # ).unsqueeze(1)
 
-        h = self.entity_embedding_layer(h).unsqueeze(1)
+        h = self.entity_embedding_layer[h].unsqueeze(1)
 
-        r = self.rel_embedding_layer(r).unsqueeze(1)
+        r = self.rel_embedding_layer[r].unsqueeze(1)
 
 
         projected_t = project_t([h, r],device)
@@ -241,7 +241,10 @@ def find_kNN(t_vec_and_embed_matrix,topk = param.k):
     """
     predicted_t_vec = torch.squeeze(t_vec_and_embed_matrix[0])  # shape (batch_size=1, 1, dim) -> (dim,)
     embedding_matrix = t_vec_and_embed_matrix[1]
-    distance = torch.norm(torch.sub(embedding_matrix.weight, predicted_t_vec), dim=1)
+    try:
+        distance = torch.norm(torch.sub(embedding_matrix.weight, predicted_t_vec), dim=1)
+    except:
+        distance = torch.norm(torch.sub(embedding_matrix, predicted_t_vec), dim=1)
     top_k_scores, top_k_t = torch.topk(-distance, k=topk)  # find indices of k largest score. score = neg(distance)
     return [torch.reshape(top_k_t, [1, topk]), torch.reshape(top_k_scores, [1, topk])]  # reshape to one row matrix to fit keras model output
 
diff --git a/test.py b/test.py
index c1e730a..e049ff1 100644
--- a/test.py
+++ b/test.py
@@ -28,7 +28,6 @@ from src.weightlearning import *
 import argparse
 
 
-
 def set_logger(param, model_dir):
     '''
     Write logs to checkpoint and console
@@ -120,7 +119,7 @@ def main(args):
 
 
 if __name__ == "__main__":
-    # main(parse_args())
+    main(parse_args())
     #
     # main(parse_args(['--knowledge_model', 'rotate',
     #                  '--target_language', 'ja',
@@ -129,11 +128,11 @@ if __name__ == "__main__":
     #                  ]))
 
 
-    main(parse_args(['--knowledge_model', 'transe',
-                     '--target_language', 'ja',
-                     '--model_dir', 'trained_model_kg_only/kens-transe-300/ja/',
-                     '-d', '300'
-                     ]))
+    # main(parse_args(['--knowledge_model', 'transe',
+    #                  '--target_language', 'ja',
+    #                  '--model_dir', 'trained_model_kg_only/kens-transe-300/ja/',
+    #                  '-d', '300'
+    #                  ]))
 
 
 
